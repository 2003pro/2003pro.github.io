---
title: 'LLM for beginners'
date: 2024-02-14
permalink: /posts/2024/02/llm-for-beginners/
tags:
  - large language models
  - paper collection
---


The following are two parts for helping the beginners in large language models (LLM) to get quick insight about it.

1. A collection of kernel papers of large language models for the beginners, which are aiming at helping the fresh LLMers for getting the idea of LLM techniques.
2. Several reference materials for optimization, machine learning and pre-LLM NLP techniques.

======
## 1. Kernel Paper Collection
======

------

1.1 Distributed Word Representation
------
[Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)
[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
[GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf)

1.2 Contextual Word Representations and MLM Pretraining  
------
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
[ELMo: Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
[Contextual Word Representations: A Contextual Introduction](https://arxiv.org/pdf/1902.06006.pdf)
[The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)
[Jurafsky and Martin Chapter 11 (Fine-Tuning and Masked Language Models)](https://web.stanford.edu/~jurafsky/slpdraft/11.pdf)

1.3 Generative Pretraining
------
[GPT-2: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
[GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)

1.4 Instruction Tuning and Alignment
-------
[InstructGPT: Aligning language models to follow instructions](https://openai.com/research/instruction-following)
[Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
[Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)
[Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
[Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality](https://lmsys.org/blog/2023-03-30-vicuna/)
[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

1.5 Efficient Finetuning Techniques
-------
[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)

1.6 Acceleration and Efficiency
-------
[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
[ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
[Gradient Checkpoint: Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/pdf/1604.06174.pdf)
[What is Gradient Accumulation ?](https://intuitiveshorts.substack.com/p/short-7-what-is-gradient-accumulation?utm_source=profile&utm_medium=reader2)

1.7 Deployment and Speed-Up Inference
-------
[vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180.pdf)
[Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192.pdf)


======
## 2. Reference for Basic ML techniques
======

2.1 Optimization and Neural Network Basics
------

[Stanford SLP book notes on Neural Networks, Backpropagation](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes03-neuralnets.pdf)

[HKUST Prof.Kim's PyTorchZeroToAll Tutorial](https://www.youtube.com/playlist?list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m)

[Deep Learning Practical Methodology](https://www.deeplearningbook.org/contents/guidelines.html)

------

2.2 Language Model and Neural Network Architectures
------

[Stanford CS224N notes on Language Models, RNN, GRU and LSTM ](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)

[Stanford CS224N notes on Self-Attention & Transformers  ](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)

[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

------

2.3 Word Vectors and Tokenizers
------

[Stanford CS224N notes on Word Vectors ](https://web.stanford.edu/class/cs224n/readings/cs224n_winter2023_lecture1_notes_draft.pdf)

[Huggingface Tokenizer's Summary](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece)

[Tokenizers' Chinese Summary on Zhihu](https://zhuanlan.zhihu.com/p/360290118)

