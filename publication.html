
<p><b>2022</b></p>
<a href="">Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks</a>.<br>
Xinsong Zhang, Yan Zeng, Jipeng Zhang, Hang Li.<br>
<i>arXiv, 2023. 01</a></i>, 2023<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>

<p><b>2022</b></p>
<a href="">Analogical Math Word Problems Solving with Enhanced Problem-Solution Association</a>.<br>
Zhenwen Liang, Jipeng Zhang, Xiangliang Zhang.<br>
<i>arXiv, 2022. 12</a></i>, 2022<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>

<p><b>2022</b></p>
<a href="">Generalizing Math Word Problem Solvers via Solution Diversification</a>.<br>
Zhenwen Liang, Jipeng Zhang, Lei Wang, Yan Wang, Jie Shao, Xiangliang Zhang.<br>
<i>arXiv, 2022. 12</a></i>, 2022<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>


<p><b>2022</b></p>
<a href="">X^{2}-VLM: All-In-One Pre-trained Model For Vision-Language Tasks</a>.<br>
Yan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, Wangchunshu Zhou.<br>
<i>arXiv, 2022. 11</a></i>, 2022<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>

<p><b>2022</b></p>
<a href="">Execution-based Evaluation for Data Science Code Generation Models</a>.<br>
Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan Duan, Jianfeng Gao.<br>
<i>arXiv, 2022.11</a></i>, 2022<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>

<p><b>2022</b></p>
<a href="">Non-Autoregressive Cross-Modal Coherence Modelling</a>.<br>
Yi Bin, Wenhao Shi, Jipeng Zhang, Yujuan Ding, Yang Yang, Heng Tao Shen.<br>
<i>{MM} '22: The 30th {ACM} International Conference on Multimedia</a></i>, 2022<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>


<p><b>2021</b></p>
<a href="">MWP-BERT: Numeracy-augmented pre-training for math word problem solving</a>.<br>
Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang.<br>
<i>Findings of the Association for Computational Linguistics: {NAACL}</a></i>, 2021<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>



<p><b>2020</b></p>
<a href="">Action-Centric Relation Transformer Network for Video Question Answering</a>.<br>
Jipeng Zhang, Jie Shao, Rui Cao, Lianli Gao, Xing Xu and Heng Tao shen.<br>
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</a></i>, 2020<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>


<p><b>2019</b></p>
<a href="">Teacher-Student Networks with Multiple Decoders forSolving Math Word Problem</a>.<br>
Jipeng Zhang, Roy Ka-Wei Lee, Ee-Peng Lim, Wei Qin, Lei Wang, Jie Shao and Qianru Sun.<br>
<i>International Joint Conference on Artificial Intelligence (IJCAI)</a></i>, 2020<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>
<a href="">Graph-to-Tree Learning for Solving Math Word Problems</a>.
<br>
Jipeng Zhang<sup>*</sup>, Lei Wang<sup>*</sup>, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao and Ee-Peng Lim.<br>
<i>Association for Computational Linguistics (ACL)</i>, 2020<br>
[<a href="">bib</a>]
[<a href="">code</a>]<br>
<sup>*</sup> equal contribution
<div id="div1"></div><div id="bib1" style="display:none">
<div class="bib">
<pre>

</pre>
</div>
</div>
<br>
<p><b>2018</b></p>
<a href="https://www.aclweb.org/anthology/P19-1619.pdf">Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions</a>..<br>
Jierui Li<sup>*</sup>, Lei Wang<sup>*</sup>, Jipeng Zhang, Yan Wang, Bingtian Dai and Dongxiang Zhang.<br>
<i>Association for Computational Linguistics (ACL)</i>, 2019<br>
[<a href="javascript:copy(div3, bib3)">bib</a>]
[<a href="https://github.com/lijierui/group-attention">code</a>]<br>
<sup>*</sup> equal contribution
<div id="div3"></div><div id="bib3" style="display:none">
<div class="bib">
<pre>
@inproceedings{DBLP:conf/acl/LiWZWDZ19,
  author    = {Jierui Li and
               Lei Wang and
               Jipeng Zhang and
               Yan Wang and
               Bing Tian Dai and
               Dongxiang Zhang},
  title     = {Modeling Intra-Relation in Math Word Problems with Different Functional
               Multi-Head Attentions},
  booktitle = {ACL},
  pages     = {6162--6167},
  year      = {2019}
}
</pre>
</div>
</div>

Remove hyperlink tags for all authors and keep hyperlinks for title:

<br>
<a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4697/4575">Template-Based Math Word Problem Solvers with Recursive Neural Networks</a>.<br>
Lei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu, Bingtian Dai and Heng Tao Shen.<br>
<i>Association for the Advancement of Artificial Intelligence (AAAI)</i>, 2019<br>
[<a href="javascript:copy(div5, bib5)">bib</a>]
[<a href="https://github.com/uestc-db/T-RNN">code</a>]<br>
<div id="div5"></div><div id="bib5" style="display:none">
<div class="bib">
<pre>
@inproceedings{WangZZXGDS19,
  author    = {Lei Wang and
               Dongxiang Zhang and
               Jipeng Zhang and
               Xing Xu and
               Lianli Gao and
               Bing Tian Dai and
               Heng Tao Shen},
  title     = {Template-Based Math Word Problem Solvers with Recursive Neural Networks},
  booktitle = {AAAI},
  pages     = {7144--7151},
  year      = {2019}
}
</pre>
</div>
</div>
<br>



